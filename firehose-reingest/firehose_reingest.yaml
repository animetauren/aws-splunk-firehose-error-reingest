AWSTemplateFormatVersion: "2010-09-09"
Description: Data Reingestion into Splunk in real-time using Lambda and Kinesis

Parameters:
  ReingestBucket:
    Type: String
    Description: S3 bucket that will act as Reingestion Bucket
    Default: reingestbucket     
  FailureBucket:
    Type: String
    Description: S3 bucket that will act as Reingestion Bucket
    Default: failurebucket
  ReingestLambdaFunctionName:
    Type: String
    Description: Reingest Lambda Function Name that will pick up messages from S3
    Default: reingest_lambda     
  KinesisTransformationLambdaFunctionName:
    Type: String
    Description: Kinesis Transformation Lambda Function Name that transforms messages from S3 to send to Splunk. 
    Default: kinesis_transform_lambda    
  DeliveryStreamName:
    Type: String
    Description: Name for Kinesis Data Firehose Delivery Stream
    Default: reingeststream
  CompressionFormat:
    Type: String
    Description: Compression settings for Kinesis Data Firehose Delivery Stream
    Default: UNCOMPRESSED
  SplunkClusterEndpoint:
    Type: String
    Description: Splunk HEC Endpoint url
  SplunkHECAuthenticationToken:
    Type: String
    Description: Splunk HEC Authentication Token

Resources:
#-------------------------------------------------------------------------------
# IAM RESOURCES
#-------------------------------------------------------------------------------

  KinesisTransformationLambdaIAMRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Policies:
        - PolicyName: KinesisTransformationLambdaPoliy
          PolicyDocument: 
            Version: "2012-10-17"
            Statement:
                - Effect: Allow
                  Action:
                    - logs:CreateLogGroup
                    - firehose:PutRecordBatch
                  Resource:
                    - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*
                    - !Sub arn:aws:firehose:${AWS::Region}:${AWS::AccountId}:deliverystream/${DeliveryStreamName}
                - Effect: Allow
                  Action:
                    - logs:CreateLogGroup
                    - logs:PutLogEvents
                  Resource:
                    - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*
                    - !Sub arn:aws:firehose:${AWS::Region}:${AWS::AccountId}:deliverystream/${DeliveryStreamName}                      

  S3ReingestLambdaIAMRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /
      Policies:
        - PolicyName: S3ReingestLambdaPolicy
          PolicyDocument:               
            Version: "2012-10-17"
            Statement:
                - Effect: Allow
                  Action:
                    - firehose:PutRecordBatch
                  Resource: 
                    - !Sub arn:aws:firehose:${AWS::Region}:${AWS::AccountId}:deliverystream/${DeliveryStreamName}
                - Effect: Allow
                  Action:
                    - s3:DeleteObject
                    - s3:PutObject
                  Resource:
                    - !Sub arn:aws:s3:::${ReingestBucket}/* 
                - Effect: Allow
                  Action:
                    - logs:*
                  Resource:
                    - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*
                - Effect: Allow
                  Action:
                    - s3:Get*
                    - s3:List*
                    - s3-object-lambda:List*
                    - s3-object-lambda:Get*
                  Resource:
                    - !Sub arn:aws:s3:::${ReingestBucket}/*
                - Effect: Allow
                  Action:
                    - 'lambda:CreateLogStream'
                    - 'lambda:PutLogEvents'
                  Resource:
                    - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${ReingestLambdaFunctionName}

  FirehoseReIngestDeliveryRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Sid: ''
            Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: 'sts:AssumeRole'
            Condition:
              StringEquals:
                'sts:ExternalId': !Ref 'AWS::AccountId'
      Policies:
        - PolicyName: FirehoseDeliveryPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - 's3:AbortMultipartUpload'
                  - 's3:GetBucketLocation'
                  - 's3:GetObject'
                  - 's3:ListBucket'
                  - 's3:ListBucketMultipartUploads'
                  - 's3:PutObject'
                Resource:
                  - !Sub arn:aws:s3:::${FailureBucket}/*
              - Effect: Allow
                Action:
                  - 'firehose:PutRecord'
                  - 'firehose:PutRecordBatch'
                Resource: 
                  - !Sub arn:aws:firehose:${AWS::Region}:${AWS::AccountId}:deliverystream/${DeliveryStreamName}
              - Effect: Allow
                Action:
                  - 'lambda:InvokeFunction'
                  - 'lambda:GetFunctionConfiguration'
                Resource: 
                  - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${KinesisTransformationLambdaFunction}

#-------------------------------------------------------------------------------
# S3 RESOURCES
#-------------------------------------------------------------------------------

  ReingestFirehoseBucket:
    Type: 'AWS::S3::Bucket'
    Properties:
      BucketName: !Ref ReingestBucket
      NotificationConfiguration:
        LambdaConfigurations:
          - Function: !GetAtt S3ReingestLambdaFunction.Arn
            Event: 's3:ObjectCreated:Post'
            Filter:
                S3Key:
                  Rules:
                    - Name: prefix
                      Value: 'splunk-failed/'
      AccessControl: Private
      PublicAccessBlockConfiguration:
        BlockPublicAcls: True
        BlockPublicPolicy: True
        IgnorePublicAcls: True
        RestrictPublicBuckets: True

  FirehoseFailureBucket:
    Type: 'AWS::S3::Bucket'
    Properties:
      BucketName: !Join
        - "-"
        - - !Ref FailureBucket
          - !Select
            - 0
            - !Split
              - "-"
              - !Select
                - 2
                - !Split
                  - "/"
                  - !Ref "AWS::StackId"
      AccessControl: Private
      PublicAccessBlockConfiguration:
        BlockPublicAcls: True
        BlockPublicPolicy: True
        IgnorePublicAcls: True
        RestrictPublicBuckets: True        

#-------------------------------------------------------------------------------
# LAMBDA RESOURCES
#-------------------------------------------------------------------------------

  KinesisTransformationLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${KinesisTransformationLambdaFunctionName}
      Code:
        ZipFile: |
          # Lambda function for reading from Firehose's "Splashback" Backup S3 Bucket.
          # Function will read from S3 and write back to Firehose. 
          # Ensure that the appropriate lambda function is enabled on the Firehose, otherwise the events will lose "source" and
          # also potentially continiously loop if no connection to HEC is restored
          # Function will Drop any unsent Events back into the ORIGINATING S3 Bucket. (after timeout)
          # Uses 3 Environment variables - firehose, region, and max_ingest

          import urllib.robotparser, boto3, json, base64, os, gzip
          from io import BytesIO
          from gzip import GzipFile

          s3_client=boto3.client('s3')

          def putRecordsToFirehoseStream(streamName, records, client, attemptsMade, maxAttempts):
              failedRecords = []
              codes = []
              errMsg = ''
              # if put_record_batch throws for whatever reason, response['xx'] will error out, adding a check for a valid
              # response will prevent this
              response = None
              try:
                  response = client.put_record_batch(DeliveryStreamName=streamName, Records=records)
              except Exception as e:
                  failedRecords = records
                  errMsg = str(e)

              # if there are no failedRecords (put_record_batch succeeded), iterate over the response to gather results
              if not failedRecords and response and response['FailedPutCount'] > 0:
                  for idx, res in enumerate(response['RequestResponses']):
                      # (if the result does not have a key 'ErrorCode' OR if it does and is empty) => we do not need to re-ingest
                      if 'ErrorCode' not in res or not res['ErrorCode']:
                          continue

                      if 'ServiceUnavailableException' in res['ErrorMessage']:
                          print("ServiceUnavailableException")
                          print("The service (Kinesis Firehose) is unavailable.\n Back off and retry the operation.\n If you continue to see the exception,\n throughput limits for the delivery stream may have been exceeded.")

                      codes.append(res['ErrorCode'])
                      failedRecords.append(records[idx])

                  errMsg = 'Individual error codes: ' + ','.join(codes)

              if len(failedRecords) > 0:
                  if attemptsMade + 1 < maxAttempts:
                      print('Some records failed while calling PutRecordBatch to Firehose stream, retrying. %s' % (errMsg))
                      putRecordsToFirehoseStream(streamName, failedRecords, client, attemptsMade + 1, maxAttempts)
                  else:
                      raise RuntimeError('Could not put records after %s attempts. %s' % (str(maxAttempts), errMsg))

          def lambda_handler(event, context):
              
              bucket=event['Records'][0]['s3']['bucket']['name']
              key=urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
              
              try:
                  firehose_dest=os.environ['Firehose']
              except:
                  print('Firehose environment variable is not set!!')
                  return
              try:
                  region=os.environ['Region']
              except:
                  print('Region variable is not set!!')
                  return
              try:
                  max_ingest=int(os.environ['max_ingest'])
                  if max_ingest>10:
                      max_ingest=9 #do not ingest more than 9 times, even if set in environment
              except:
                  max_ingest=2
              try:
                  if os.environ['Cleanup'] == 'True':
                      clean_s3bucket = True
                  else:
                      clean_s3bucket = False            
              except:
                  clean_s3bucket = False             
              try:
                  client = boto3.client('firehose', region_name=region)
                  streamName=firehose_dest

                  response=s3_client.get_object(Bucket=bucket, Key=key)

                  if key.endswith('.gz'):
                      bytestream = BytesIO(response['Body'].read())
                      text=GzipFile(None, 'rb', fileobj=bytestream).read().decode('utf-8')
                  else:
                      text=response["Body"].read().decode()
                  
                  payload=""
                  recordBatch=[]
                  reingestjson={}
                  destFH=0
                  destS3=0
                  s3payload={}
                  reingest_count=1
                  
                  for line in text.split("\n"): #process every 'batch'
                      dest='FH' #default destination will be FH
                      if len(line)>0:
                          data=json.loads(line)
                          base64_message = data['rawData']
                          base64_bytes = base64_message.encode('utf-8')
                          message_bytes = base64.b64decode(base64_bytes)                
                          #We need to check if gzip was enabled. If not we decode b64 as usual, if not we need to decompress gzipped base64 string
                          try:
                              message = message_bytes.decode('utf-8')
                          except (UnicodeDecodeError, AttributeError):
                              message = (gzip.decompress(message_bytes)).decode('utf-8')                    
                              pass

                          for messageline in message.split("\n"): #process every line of the batch
                              if len(messageline)>0:
                              
                                  try:
                                      dest='FH'
                                      try:
                                          jsondata=json.loads(messageline)
                                      except Exception as e:
                                          print("Error: Malformed event message, not valid JSON. See below for event message:")
                                          print(messageline)
                                          return

                                      #get the metadata
                                      if jsondata.get('source')!=None:
                                          source=jsondata.get('source')
                                      else:
                                          source='aws:reingested'

                                      if jsondata.get('sourcetype')!=None:
                                          st=jsondata.get('sourcetype')
                                      else:
                                          st='aws:firehose'
                                      
                                      fieldsreingest={}
                                      
                                      if jsondata.get('fields')!=None:
                                          
                                          fieldsreingest=jsondata.get('fields') #get reingest fields
                                          reingest_count=int(fieldsreingest.get('reingest'))+1 #advance counter
                                          
                                          fieldsreingest['reingest']=str(reingest_count)
                                          mbucket=fieldsreingest["frombucket"]
                                      else: #fields not set, first reingest
                                          
                                          fieldsreingest["reingest"]='1'
                                          fieldsreingest["frombucket"]=bucket
                                          mbucket=bucket
                                          reingest_count=1

                                      if reingest_count > max_ingest:
                                          #package up for S3
                                          destS3+=1
                                          if s3payload.get(mbucket)==None:
                                              s3payload[mbucket]=json.dumps(jsondata.get('event'))+'\n'
                                          else:
                                              s3payload[mbucket]=s3payload[mbucket]+json.dumps(jsondata.get('event'))+'\n'
                                          dest='S3'
                                      else:
                                          if jsondata.get('time')!=None:
                                              reingestjson= {'sourcetype':st, 'source':source, 'event':jsondata.get('event'), 'fields': fieldsreingest, 'time':jsondata.get('time')}
                                          else:
                                              reingestjson= {'sourcetype':st, 'source':source, 'event':jsondata.get('event'), 'fields': fieldsreingest}

                                  except Exception as e:
                                      print(e)
                                      reingestjson= {'reingest':jsondata['fields'], 'sourcetype':jsondata['sourcetype'], 'source':'reingest:'+str(reingest_count), 'detail-type':'Reingested Firehose Message','event':jsondata['event']}
                                  
                                  
                                  if dest=='FH':
                                      messageline=json.dumps(reingestjson)
                                      message_bytes=messageline.encode('utf-8')
                                      recordBatch.append({'Data':message_bytes})
                                      destFH+=1
                                      if destFH>499:
                                          #flush max batch 
                                          putRecordsToFirehoseStream(streamName, recordBatch, client, attemptsMade=0, maxAttempts=20)
                                          destFH=0
                                          recordBatch=[]
                  
                  #flush all        
                  if destFH>0: 
                      putRecordsToFirehoseStream(streamName, recordBatch, client, attemptsMade=0, maxAttempts=20)
                  if destS3>0:
                      print('Already re-ingested more than max attempts, will write to S3 to prevent looping')
                      file_name = key
                      s3_path = "SplashbackRawFailed/" + file_name
                      for wbucket in s3payload:
                          bucket_name=wbucket
                          print('writing to bucket:',bucket_name, ' s3_key:', s3_path)
                          s3write = boto3.resource("s3")
                          s3write.Bucket(bucket_name).put_object(Key=s3_path, Body=s3payload[wbucket].encode("utf-8")) 
                  
              except Exception as e:
                  #Print error message, and send failure notification
                  print(e)           
                  raise e

              #Checking to see if we want to cleanup the old data that has been re-ingested.                
              if clean_s3bucket:
                  response = s3_client.delete_object(Bucket=bucket, Key=key)
                  print("Cleaning Bucket: {} and Path: {}".format(bucket, key))
              else:
                  print("Cleanup not required")

              return 'Success!'
      Description: !Sub "Data transformation Lambda for ${DeliveryStreamName}"
      Handler: index.handler
      MemorySize: 128
      Role: !GetAtt KinesisTransformationLambdaIAMRole.Arn
      Runtime: python3.9
      Timeout: 60

  KinesisTransformationLambdaFunctionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${KinesisTransformationLambdaFunctionName}"
      RetentionInDays: 7

  KinesisTransformationLambdaFunctionPermission:
    Type: AWS::Lambda::Permission
    DependsOn: KinesisTransformationLambdaFunction
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref KinesisTransformationLambdaFunctionName
      Principal: firehose.amazonaws.com

  S3ReingestLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${ReingestLambdaFunctionName}
      Code:
        ZipFile: |
          """
          This Lambda function is to be added to a Kinesis Firehose Data Stream.
          Only For processing data sent to Firehose by Re-ingest process. Note: Records are not converted/transformed.

          """

          import base64
          import boto3
          import os
          import json

          def processRecords(records):
              
              for r in records:
                  
                  data = json.loads(base64.b64decode(r['data']))
                  recId = r['recordId']
                  return_event = {}

                  return_event['sourcetype'] = data['sourcetype']
                  return_event['source'] = data['source']
                  return_event['event'] = data['event']
                  
                  if data.get('time')!=None:
                      return_event['time'] = data['time']
                  if data.get('fields')!=None:
                      return_event['fields'] = data['fields']

                  data = base64.b64encode(json.dumps(return_event).encode('utf-8')).decode()

                  yield {
                      'data': data,
                      'result': 'Ok',
                      'recordId': recId
                  }


          def putRecordsToFirehoseStream(streamName, records, client, attemptsMade, maxAttempts):
              failedRecords = []
              codes = []
              errMsg = ''
              # if put_record_batch throws for whatever reason, response['xx'] will error out, adding a check for a valid
              # response will prevent this
              response = None
              try:
                  response = client.put_record_batch(DeliveryStreamName=streamName, Records=records)
              except Exception as e:
                  failedRecords = records
                  errMsg = str(e)

              # if there are no failedRecords (put_record_batch succeeded), iterate over the response to gather results
              if not failedRecords and response and response['FailedPutCount'] > 0:
                  for idx, res in enumerate(response['RequestResponses']):
                      # (if the result does not have a key 'ErrorCode' OR if it does and is empty) => we do not need to re-ingest
                      if 'ErrorCode' not in res or not res['ErrorCode']:
                          continue
                      
                      if 'ServiceUnavailableException' in res['ErrorMessage']:
                          print("ServiceUnavailableException")
                          print("The service (Kinesis Firehose) is unavailable.\n Back off and retry the operation.\n If you continue to see the exception,\n throughput limits for the delivery stream may have been exceeded.")
                      
                      codes.append(res['ErrorCode'])
                      failedRecords.append(records[idx])

                  errMsg = 'Individual error codes: ' + ','.join(codes)

              if len(failedRecords) > 0:
                  if attemptsMade + 1 < maxAttempts:
                      print('Some records failed while calling PutRecordBatch to Firehose stream, retrying. %s' % (errMsg))
                      putRecordsToFirehoseStream(streamName, failedRecords, client, attemptsMade + 1, maxAttempts)
                  else:
                      raise RuntimeError('Could not put records after %s attempts. %s' % (str(maxAttempts), errMsg))


          def putRecordsToKinesisStream(streamName, records, client, attemptsMade, maxAttempts):
              failedRecords = []
              codes = []
              errMsg = ''
              # if put_records throws for whatever reason, response['xx'] will error out, adding a check for a valid
              # response will prevent this
              response = None
              try:
                  response = client.put_records(StreamName=streamName, Records=records)
              except Exception as e:
                  failedRecords = records
                  errMsg = str(e)

              # if there are no failedRecords (put_record_batch succeeded), iterate over the response to gather results
              if not failedRecords and response and response['FailedRecordCount'] > 0:
                  for idx, res in enumerate(response['Records']):
                      # (if the result does not have a key 'ErrorCode' OR if it does and is empty) => we do not need to re-ingest
                      if 'ErrorCode' not in res or not res['ErrorCode']:
                          continue
                      
                      if 'ServiceUnavailableException' in res['ErrorMessage']:
                          print("ServiceUnavailableException")
                          print("The service (Kinesis Stream) is unavailable.\n Back off and retry the operation.\n If you continue to see the exception,\n throughput limits for the delivery stream may have been exceeded.")

                      codes.append(res['ErrorCode'])
                      failedRecords.append(records[idx])

                  errMsg = 'Individual error codes: ' + ','.join(codes)

              if len(failedRecords) > 0:
                  if attemptsMade + 1 < maxAttempts:
                      print('Some records failed while calling PutRecords to Kinesis stream, retrying. %s' % (errMsg))
                      putRecordsToKinesisStream(streamName, failedRecords, client, attemptsMade + 1, maxAttempts)
                  else:
                      raise RuntimeError('Could not put records after %s attempts. %s' % (str(maxAttempts), errMsg))


          def createReingestionRecord(isSas, originalRecord):
              if isSas:
                  return {'data': base64.b64decode(originalRecord['data']), 'partitionKey': originalRecord['kinesisRecordMetadata']['partitionKey']}
              else:
                  return {'data': base64.b64decode(originalRecord['data'])}

          def getReingestionRecord(isSas, reIngestionRecord):
              if isSas:
                  return {'Data': reIngestionRecord['data'], 'PartitionKey': reIngestionRecord['partitionKey']}
              else:
                  return {'Data': reIngestionRecord['data']}        

          def lambda_handler(event, context):

              isSas = 'sourceKinesisStreamArn' in event
              streamARN = event['sourceKinesisStreamArn'] if isSas else event['deliveryStreamArn']
              region = streamARN.split(':')[3]
              streamName = streamARN.split('/')[1]
              records = list(processRecords(event['records']))
              projectedSize = 0
              dataByRecordId = {rec['recordId']: createReingestionRecord(isSas, rec) for rec in event['records']}
              putRecordBatches = []
              recordsToReingest = []
              totalRecordsToBeReingested = 0

              for idx, rec in enumerate(records):
                  if rec['result'] != 'Ok':
                      continue
                  projectedSize += len(rec['data']) + len(rec['recordId'])
                  # 6000000 instead of 6291456 to leave ample headroom for the stuff we didn't account for
                  if projectedSize > 6000000:
                      totalRecordsToBeReingested += 1
                      recordsToReingest.append(
                          getReingestionRecord(isSas, dataByRecordId[rec['recordId']])
                      )
                      records[idx]['result'] = 'Dropped'
                      del(records[idx]['data'])

                  # split out the record batches into multiple groups, 500 records at max per group
                  if len(recordsToReingest) == 500:
                      putRecordBatches.append(recordsToReingest)
                      recordsToReingest = []

              if len(recordsToReingest) > 0:
                  # add the last batch
                  putRecordBatches.append(recordsToReingest)

              # iterate and call putRecordBatch for each group
              recordsReingestedSoFar = 0
              if len(putRecordBatches) > 0:
                  print("Starting process to batch ingest records into Kinesis Firehose....")
                  client = boto3.client('kinesis', region_name=region) if isSas else boto3.client('firehose', region_name=region)
                  for recordBatch in putRecordBatches:
                      if isSas:
                          putRecordsToKinesisStream(streamName, recordBatch, client, attemptsMade=0, maxAttempts=20)
                      else:
                          putRecordsToFirehoseStream(streamName, recordBatch, client, attemptsMade=0, maxAttempts=20)
                          recordsReingestedSoFar += len(recordBatch)         
                      print('Batched ingested %d/%d records out of %d' % (recordsReingestedSoFar, totalRecordsToBeReingested, len(event['records'])))
              else:
                  print("Completed Data Transformation.")
                  print('No records need to be batch ingested into Kinesis Firehose.')

              return {"records": records}   
      Description: !Sub "Data transformation Lambda for ${DeliveryStreamName}"
      Handler: index.handler
      MemorySize: 128
      Role: !GetAtt S3ReingestLambdaIAMRole.Arn
      Runtime: python3.9
      Timeout: 60

  S3ReingestLambdaFunctionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${ReingestLambdaFunctionName}"
      RetentionInDays: 7

  S3ReingestLambdaFunctionPermission:
    Type: AWS::Lambda::Permission
    DependsOn: S3ReingestLambdaFunction
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref ReingestLambdaFunctionName
      Principal: s3.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'      
      SourceArn: !Sub arn:aws:s3:::${ReingestBucket} 
              
#-------------------------------------------------------------------------------
# KINESIS RESOURCES
#-------------------------------------------------------------------------------

  Firehose:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties: 
      DeliveryStreamName: !Ref DeliveryStreamName
      DeliveryStreamType: DirectPut
      SplunkDestinationConfiguration:
        HECAcknowledgmentTimeoutInSeconds: 180
        HECEndpoint: !Ref SplunkClusterEndpoint
        HECEndpointType: Event
        HECToken: !Ref SplunkHECAuthenticationToken
        ProcessingConfiguration: 
          Enabled: True
          Processors:
            - Parameters: 
              - ParameterName: LambdaArn
                ParameterValue: !GetAtt KinesisTransformationLambdaFunction.Arn
              - ParameterName: RoleArn
                ParameterValue: !GetAtt KinesisTransformationLambdaIAMRole.Arn                
              Type: Lambda
        RetryOptions: 
          DurationInSeconds: 600
        S3BackupMode: FailedEventsOnly
        S3Configuration: 
          BucketARN: !GetAtt FirehoseFailureBucket.Arn
          BufferingHints: 
            IntervalInSeconds: 300
            SizeInMBs: 5
          CompressionFormat: !Ref CompressionFormat
          RoleARN: !GetAtt FirehoseReIngestDeliveryRole.Arn